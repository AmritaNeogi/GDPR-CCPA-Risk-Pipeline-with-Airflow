<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Data Protection Report</title>
	<atom:link href="https://www.dataprotectionreport.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.dataprotectionreport.com/</link>
	<description>Data protection legal insight at the speed of technology</description>
	<lastBuildDate>Tue, 15 Jul 2025 13:44:15 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.8.1&amp;lxb_maple_bar_source=lxb_maple_bar_source</generator>

<image>
	<url>https://dataprotectionreport.nortonroseplatform.com/wp-content/uploads/sites/15/2024/02/cropped-admin-ajax-1-32x32.png</url>
	<title>Data Protection Report</title>
	<link>https://www.dataprotectionreport.com/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Canada’s Place in the AI Data Centre Boom </title>
		<link>https://www.dataprotectionreport.com/2025/07/canadas-place-in-the-ai-data-centre-boom/</link>
		
		<dc:creator><![CDATA[Imran Ahmad (CA), Domenic Presta (CA) and Sandeep Patel]]></dc:creator>
		<pubDate>Mon, 14 Jul 2025 18:13:12 +0000</pubDate>
				<category><![CDATA[Artificial Intelligence]]></category>
		<guid isPermaLink="false">https://www.dataprotectionreport.com/?p=6595</guid>

					<description><![CDATA[As artificial intelligence (AI) continues to grow in capability and adoption, the technology infrastructure required to support it is rapidly expanding. The rise of AI has led to the development of specialized data centres built to handle the high computational demands associated with AI workloads.   <a href="https://www.dataprotectionreport.com/2025/07/canadas-place-in-the-ai-data-centre-boom/">Continue Reading</a>]]></description>
										<content:encoded><![CDATA[<img style=" max-width: 100%; height: auto; " width="800" height="422" src="https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1.jpg" class="attachment-lxb_af_1_of_1 size-lxb_af_1_of_1 wp-post-image" alt="Motherboard circuit" decoding="async" fetchpriority="high" srcset="https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1.jpg 800w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-320x169.jpg 320w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-656x346.jpg 656w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-240x127.jpg 240w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-768x405.jpg 768w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-40x21.jpg 40w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-80x42.jpg 80w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-160x84.jpg 160w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-550x290.jpg 550w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-367x194.jpg 367w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-734x387.jpg 734w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-275x145.jpg 275w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-220x116.jpg 220w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-440x232.jpg 440w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-660x348.jpg 660w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-184x97.jpg 184w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-138x73.jpg 138w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-413x218.jpg 413w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-688x363.jpg 688w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-123x65.jpg 123w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-110x58.jpg 110w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-330x174.jpg 330w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-300x158.jpg 300w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-600x317.jpg 600w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-207x109.jpg 207w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-344x181.jpg 344w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-55x29.jpg 55w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-71x37.jpg 71w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2023/11/Digital-jpg-AdobeStock_323101803-1-102x54.jpg 102w" sizes="(max-width: 800px) 100vw, 800px" />

<p>As artificial intelligence (AI) continues to grow in capability and adoption, the technology infrastructure required to support it is rapidly expanding. The rise of AI has led to the development of specialized data centres built to handle the high computational demands associated with AI workloads.</p>



<p>In this post, we explore the core infrastructure of AI data centres and analyze how Canada’s energy infrastructure, policy landscape, and environment position Canada as a desirable location for AI data centre development.</p>



<h3 class="wp-block-heading"><strong>What Is an AI Data Centre?</strong></h3>



<p>A data centre is a physical facility that houses computer systems used to store, process, and manage data. Traditional data centres&nbsp;contain many of the same components as an AI data centre, such as servers, storage systems, and network infrastructure; however, these components operate at a much higher capacity in an AI data centre to meet the speed and processing demands of AI applications.</p>



<h3 class="wp-block-heading"><strong>Power Requirements</strong></h3>



<p>AI data centres require large amounts of electricity to power high-performance hardware such as graphics processing units (GPUs). As demand for AI and cloud services grows, this is driving a sharp rise in energy use by data centres. In 2024, it was estimated that the transmission of data (including associated infrastructure) consumes 260 to 340 TWh per year, which represents roughly 1% to 1.4% of the electricity used globally.&nbsp; According to the International Energy Agency, this figure is expected to more than double by 2030.</p>



<p>In recognition of the high energy demands of AI data centres, governments and industries worldwide are taking steps to ensure their continued competitiveness in AI.</p>



<p>In the United States, some of the largest technology companies have already signed on to long-term power agreements with US energy providers to secure reliable electricity.</p>



<p>Canada offers a competitive advantage with its abundant energy sources that can provide continuous, reliable power to data centres around the clock. Alberta, in particular, could be an attractive option for investors, since its deregulated electricity market may make it easier for technology companies to establish long-term power agreements with local energy providers.</p>



<h3 class="wp-block-heading"><strong>Cooling Requirements</strong></h3>



<p>In addition to high electricity demands, AI data centres also generate significant heat as a byproduct of powering AI-related hardware. Managing this heat through cooling systems such as liquid cooling technology is critical to maintain system performance and prevent hardware failure. Canada’s cooler ambient temperatures could help reduce the need for these energy-intensive cooling systems, thereby resulting in lower operational costs.&nbsp; &nbsp;&nbsp;</p>



<h3 class="wp-block-heading"><strong>Canadian Sovereign AI Compute Strategy</strong></h3>



<p>The government of Canada is showing its commitment to developing AI data centres and plans to invest $2 billion over five years to launch new initiatives that will provide companies and researchers with the tools they need to compete in the global AI data centre race. In December 2024, the Government of Canada launched the Canadian Sovereign AI Compute Strategy, which sets out the plan for how this funding will be allocated.</p>



<p>The strategy is built around three core objectives:</p>



<ol class="wp-block-list">
<li><strong>Mobilize private sector investment</strong>.</li>



<li><strong>Build public supercomputing infrastructure</strong>.</li>



<li><strong>Establish an “AI Compute Access Fund.”</strong></li>
</ol>



<p><strong>(1) Mobilize private sector investment</strong></p>



<p>The federal government is investing up to $700&nbsp;million through the AI Compute Challenge to support projects that establish fully integrated AI data-centre solutions. Commercial entities, industry consortia and industry-academia partnerships are eligible to apply for funding. Funding will be prioritized for projects that meet the following objectives:</p>



<ul class="wp-block-list">
<li>Build out or expand the capacity of commercial AI-specific data centres in Canada.</li>



<li>Provide flexible and affordable compute offerings.</li>



<li>Contribute to anchoring or growing Canadian AI champions.</li>



<li>Advance innovative and sustainable compute solutions.</li>
</ul>



<p><strong>(2) Build public supercomputing infrastructure</strong></p>



<p>The federal government is investing up to $1&nbsp;billion to build public supercomputing infrastructure to help meet the needs of researchers, governments and industry, including those relating to AI. The investment includes:</p>



<ul class="wp-block-list">
<li>$705 million in a new AI supercomputing system through the AI Sovereign Compute Infrastructure Program.&nbsp; Under this program, builders are able to submit a statement of interest to the Government of Canada to assist with infrastructure development. The government will also launch a full proposal phase shortly that will further refine the program requirements.</li>



<li>$200 million will be provided to augment existing public compute infrastructure.</li>
</ul>



<p><strong>(3) Establish an “AI Compute Access Fund”</strong></p>



<p>The federal government is investing a further $300 million into the “AI Compute Access Fund” to support the purchase of AI compute resources by Canadian innovators and businesses. To be eligible for funding, a participant must be a Canadian-registered for-profit company developing AI products or services and must have fewer than 500 full-time equivalent employees.</p>



<h3 class="wp-block-heading"><strong>Conclusion</strong></h3>



<p>As AI advances, investment in data centres is increasing to meet the associated growing infrastructure demands. Based on recent trends, capital expenditures in the trillions of dollars are expected over the next few years to support the global buildout of new data centres. With its abundant energy resources, favourable natural environment, and strong government backing for AI, Canada is well positioned to play a leading role in the global AI infrastructure landscape.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>The Healthline Order: Privacy law grows teeth</title>
		<link>https://www.dataprotectionreport.com/2025/07/the-healthline-order-privacy-law-grows-teeth/</link>
		
		<dc:creator><![CDATA[Steve Roosa (US), Philip Hodgkins (US) and Susan Ross (US)]]></dc:creator>
		<pubDate>Fri, 11 Jul 2025 15:14:14 +0000</pubDate>
				<category><![CDATA[CCPA]]></category>
		<category><![CDATA[Privacy]]></category>
		<category><![CDATA[Privacy law]]></category>
		<category><![CDATA[CCPA Settlement]]></category>
		<category><![CDATA[Consumer Privacy Enforcement]]></category>
		<category><![CDATA[Healthline Order]]></category>
		<category><![CDATA[NT Analyzer]]></category>
		<category><![CDATA[privacy law]]></category>
		<guid isPermaLink="false">https://www.dataprotectionreport.com/?p=6598</guid>

					<description><![CDATA[The proposed $1.55 million CCPA settlement with Healthline is not just the largest of its kind to date – it is, more importantly, it marks a pivotal evolution in how American regulators are approaching consumer privacy enforcement. The facts are instructive. Healthline, a well-established provider of health-related content, allowed third party tracking tools to operate... <a href="https://www.dataprotectionreport.com/2025/07/the-healthline-order-privacy-law-grows-teeth/">Continue Reading</a>]]></description>
										<content:encoded><![CDATA[

<p>The proposed $1.55 million CCPA settlement with Healthline is not just the largest of its kind to date – it is, more importantly, it marks a pivotal evolution in how American regulators are approaching consumer privacy enforcement.</p>



<p>The facts are instructive. Healthline, a well-established provider of health-related content, allowed third party tracking tools to operate on sensitive pages – articles concerning deeply personal topics, such as multiple sclerosis, HIV, and Crohn’s disease, among others. These tools captured and transmitted browsing data, including page titles and other identifiers, to data brokers and advertisers. As a result, users were targeted with highly specific pharmaceutical ads and, in some instances, found their browsing activity reflected in third-party consumer profiles.</p>



<p>To the California Attorney General, this was more than a technical afterthought; it was a violation of the CCPA’s prohibitions on the sale and sharing of sensitive personal information – particularly when done without meaningful disclosure or consent.</p>



<p>But the most important takeaway for corporate legal and compliance teams is what the proposed order <strong>requires</strong> beyond the large financial penalty. In summary, Healthline must:</p>



<ul class="wp-block-list">
<li>Honor <strong>Global Privacy Control (GPC) </strong>signals across its platforms;</li>



<li>Implement a formal <strong>review and audit of all vendor contracts</strong> that involve personal information;</li>



<li>Ensure <strong>purpose limitations and audit rights</strong> are expressly defined in third party agreements;</li>



<li>Conduct <strong>regular compliance assessments</strong> of its adtech partners.</li>
</ul>



<p>This is a significant shift. Regulators are no longer satisfied with generalized privacy policies and practices or rote industry frameworks. They are demanding operational alignment between what a company says and what it actually does.</p>



<p>With respect to vendor contracts, the regulations warn: &nbsp;“The Business Purpose shall not be described in generic terms, such as referencing the entire contract generally.&nbsp; The description shall be specific.” &nbsp;Regulations § 7051(a)(2).&nbsp; The California Attorney General’s complaint in Paragraph 25 found these descriptions non-compliant:&nbsp;</p>



<p>rather than list the limited and specified purposes for using personal information, one contract said that the recipient could use the data for &#8220;any&nbsp; business purpose.&#8221;&nbsp; Another said it could use the data for any &#8220;internal use&#8221; inuring to the recipient&#8217;s &#8220;direct benefit&#8221;&nbsp; Another said that personal information would be processed &#8220;for the purposes contemplated&#8221; in the agreement, &#8220;or as otherwise agreed to in writing by the parties,&#8221; but the contract did not specify what those contemplated purposes were.</p>



<p>For companies operating at scale – especially those in content, media, health, and ad-supported business models – this case serves as a clear signal: <strong>your data ecosystem is only as compliant as your weakest contractual and technical control.</strong></p>



<p>At Norton Rose Fulbright, we help clients navigate the legal, operational, and reputational risks at the intersection of consumer privacy, digital advertising, and data governance. If your organization is engaging in behavioral advertising, using third party tracking technology, or is processing sensitive content &#8211; even indirectly – it is imperative to evaluate whether your consent mechanisms, opt-outs, and contracts are keeping pace with this new enforcement reality.</p>



<p>The Healthline matter is not an anomaly – it’s a roadmap. To that end, Norton Rose Fulbright offers a comprehensive solution: <strong><a href="https://www.ntanalyzer.com/">NT Analyzer</a></strong>, a proprietary privacy analytics assessment platform built to identify and remediate CCPA and global privacy law risks. NT Analyzer scans your websites and digital properties to uncover hidden trackers, test opt-out functionality, and map data flows to third parties. We pair this technical analysis with legal advice tailored to your risk profile – helping you to align contracts, privacy notices, and data uses.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Do your technology and outsourcing contracts properly address liability for cyber incidents?</title>
		<link>https://www.dataprotectionreport.com/2025/07/do-your-technology-and-outsourcing-contracts-properly-address-liability-for-cyber-incidents/</link>
		
		<dc:creator><![CDATA[James Russell and Kerri Gevers (SG)]]></dc:creator>
		<pubDate>Wed, 02 Jul 2025 08:07:58 +0000</pubDate>
				<category><![CDATA[Cybersecurity]]></category>
		<category><![CDATA[Data Protection]]></category>
		<guid isPermaLink="false">https://www.dataprotectionreport.com/?p=6589</guid>

					<description><![CDATA[Most incidents handled by our Norton Rose Fulbright cyber team originate from the customer’s service provider. In many cases it is the service provider’s systems, infrastructure and environment which proves to be the most vulnerable to cyber breaches and security issues. Continue reading <a href="https://www.dataprotectionreport.com/2025/07/do-your-technology-and-outsourcing-contracts-properly-address-liability-for-cyber-incidents/">Continue Reading</a>]]></description>
										<content:encoded><![CDATA[

<p>Most incidents handled by our Norton Rose Fulbright cyber team originate from the customer’s service provider. In many cases it is the service provider’s systems, infrastructure and environment which proves to be the most vulnerable to cyber breaches and security issues.</p>



<p><a href="https://www.nortonrosefulbright.com/en-gb/knowledge/publications/7c5553d5/do-your-technology-and-outsourcing-contracts-properly-address-liability-for-cyber-incidents" target="_blank" rel="noreferrer noopener">Continue reading</a></p>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>AI Armageddon Series</title>
		<link>https://www.dataprotectionreport.com/2025/06/ai-armageddon-series/</link>
		
		<dc:creator><![CDATA[Steve Roosa (US)]]></dc:creator>
		<pubDate>Mon, 30 Jun 2025 17:28:07 +0000</pubDate>
				<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Privacy]]></category>
		<category><![CDATA[Privacy law]]></category>
		<category><![CDATA[A2A Protocol]]></category>
		<category><![CDATA[Agent2Agent Protocol]]></category>
		<category><![CDATA[AI Communication]]></category>
		<category><![CDATA[AI Privacy]]></category>
		<category><![CDATA[AI Tools]]></category>
		<category><![CDATA[Consumer Privacy]]></category>
		<category><![CDATA[MCP]]></category>
		<category><![CDATA[Multi Context Protocol]]></category>
		<category><![CDATA[privacy regulations]]></category>
		<category><![CDATA[Privacy Risks]]></category>
		<guid isPermaLink="false">https://www.dataprotectionreport.com/?p=6557</guid>

					<description><![CDATA[The biggest AI privacy problems no one is talking about: Installment 1: The Agent2Agent (“A2A”) Protocol In the privacy world, everyone is focused on fairness, bias, and data scraping. These issues, however, are not even among the top 3 AI privacy issues. And it’s not even close. This is not to say those issues are unimportant. Indeed, the... <a href="https://www.dataprotectionreport.com/2025/06/ai-armageddon-series/">Continue Reading</a>]]></description>
										<content:encoded><![CDATA[

<p><strong>The biggest AI privacy problems no one is talking about: Installment 1: The Agent2Agent (“A2A”) Protocol</strong></p>



<p></p>



<p>In the privacy world, everyone is focused on fairness, bias, and data scraping. These issues, however, are not even among the top 3 AI privacy issues. And it’s not even close. This is not to say those issues are unimportant. Indeed, the contrary is true. The point is instead that they won’t be the issues that are initially going to create the most significant outsized risk for businesses.</p>



<p>The most significant class of AI privacy risks, by far, arise from the wildly popular family of information-sharing agentic protocols—protocols that allow AI agents to use external tools and resources (the Multi Context Protocol or “MCP”) and that allow AI agents to talk to other AI agents (the Agent2Agent Protocol or “A2A Protocol”).&nbsp;The purpose of both these protocols is to facilitate the flow of information in and out of the primary AI agent in order to either receive additional data inputs from external resources (MCP) or to feed data to third party AI agents for the purpose of performing tasks not provided by the primary AI agent (A2A Protocol).</p>



<p>Neither MCP nor the A2A Protocol supports common consumer privacy requirements or critical enterprise privacy/security obligations (other than basic authentication and secure transmission).&nbsp;Nevertheless, engineers love these protocols since they allow AI agents to easily access external resources and third-party AI agents in an easy, plug-and-play, and standardized way. As a result, there is a 100% chance that these protocols will be baked into the first AI tools that your company will ask you to approve, either for use in enterprise environments or for consumer-facing products, services, applications, or websites.</p>



<figure style=" max-width: 100%; height: auto; " class="wp-block-image size-large"><img style=" max-width: 100%; height: auto; " decoding="async" width="656" height="438" src="https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-656x438.jpg" alt="" class="wp-image-6558" srcset="https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-656x438.jpg 656w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-320x214.jpg 320w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-240x160.jpg 240w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-768x513.jpg 768w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-40x27.jpg 40w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-80x53.jpg 80w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-160x107.jpg 160w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-550x367.jpg 550w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-367x245.jpg 367w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-734x490.jpg 734w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-275x184.jpg 275w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-825x551.jpg 825w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-220x147.jpg 220w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-440x294.jpg 440w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-660x440.jpg 660w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-880x587.jpg 880w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-184x123.jpg 184w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-917x612.jpg 917w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-138x92.jpg 138w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-413x276.jpg 413w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-688x459.jpg 688w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-123x82.jpg 123w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-110x73.jpg 110w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-330x220.jpg 330w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-300x200.jpg 300w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-600x400.jpg 600w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-207x138.jpg 207w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-344x230.jpg 344w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-55x37.jpg 55w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-71x47.jpg 71w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-81x54.jpg 81w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image.jpg 935w" sizes="(max-width: 656px) 100vw, 656px" /></figure>



<p>Today we will talk about the A2A Protocol and leave MCP for a subsequent post.&nbsp;</p>



<p>For starters, the A2A Protocol is a an open-source project by Google and its core documentation and code can be found on <a href="https://github.com/a2aproject/A2A">GitHub.</a>&nbsp;</p>



<p><strong>The Official A2A Protocol Mission</strong></p>



<p>As noted, the core purpose of the A2A Protocol, according to its own documentation, is to allow for inter-AI agent communication.&nbsp;A2A aims to “break down silos” and allow AI agents to connect with one another “across different ecosystems.”&nbsp;The A2A Protocol seeks to do this while “<em>preserving opacity,”</em> i.e. allowing AI agents to “collaborate without needing to share internal memory, proprietary logic, or specifical tool implementations…”.</p>



<p><strong>The Privacy Translation</strong></p>



<p>The A2A Protocol creates a number of critical privacy and security problems for company AI agents (regardless of whether the agent is homegrown or provided by a vendor).</p>



<ol class="wp-block-list">
<li><span style="text-decoration: underline">Invisible Sharing of Data. </span>The company AI agent will invisibly disclose and transmit data—probably including personal information—to 3rd parties to which the company AI agent connects using the A2A Protocol.</li>



<li><span style="text-decoration: underline">Hidden Implementation.</span> The company AI agent will likely not disclose to you how and when it uses the A2A Protocol.  In other words, whether the A2A Protocol is implemented in the company AI agent will not be obvious on the face of the AI agent, but instead will be buried in code.</li>



<li><span style="text-decoration: underline">Lack of Transparency by Design.</span> When the company AI agent uses the A2A Protocol, it will necessarily—as a feature of the protocol itself—be blind to the key operational privacy details of the outside AI agent to which it connects. The A2A protocol not only “abstracts away” the operational details of the outside AI agent (an “A2A server”), it enforces this as a security guarantee via technical requirements around “opacity.”  Thus, an A2A Server only makes available a very small number of representations about its operational details (via the “Agent Card” – JSON string that contains information about what the AI agent can do and how to interact with it.  See below.) but none of these have anything to do with privacy.</li>



<li><span style="text-decoration: underline">Promiscuity.</span> Google, likely inspired by its ad tech model, has designed the A2A Protocol to permit arbitrary “discovery” of A2A servers, i.e., “to allow agents to dynamically find and understand the capabilities of other agents.” See <a href="https://a2aproject.github.io/A2A/specification/#1-introduction">A2A Specification</a>. This means it is very likely that the company AI Agent will not decide until runtime which 3rd party AI agents it wants to connect to via the A2A Protocol. That may be great for flexibility, but for privacy and security, it’s an absolute nightmare for anything but the most trivial services. The reason is because it won’t permit Legal or governance mechanisms to evaluate the suitability of the third-party AI agent with respect to privacy and overarching security concerns.</li>
</ol>



<p>The maintainers of the A2A Protocol might object to the above characterization and respond by claiming that transparency, privacy, and security are provided by the “Agent Card” which every A2A server must make available to A2A client software. The Agent Card, however, is completely useless for security and privacy because:</p>



<p>(1) the “Agent Card” template for A2A servers doesn’t contain any specified fields relative to privacy (e.g. how the third party will use personal information, whether it will be shared, how long it will be stored, etc., etc.).&nbsp;Zero.&nbsp; None.&nbsp; Nada.</p>



<p>(2) the A2A server can arbitrarily and without oversight put whatever it wants on the card—it can lie with impunity.</p>



<p>The bottom line is no one is going to stop endpoints from lying on their A2A Agent Cards. Of course, for privacy purposes, it won’t even matter since the specification contains ZERO fields for privacy/data protection.&nbsp;</p>



<p>The entire A2A Specification prattles on for 9,501 words before, on the very last line of the entire document, it finally gives the most perfunctory hand wave in the general direction of privacy:</p>



<p class="has-text-align-center"><strong>Data Privacy:</strong> Adhere to all applicable privacy regulations for data exchanged in Message and Artifact parts. Minimize sensitive data transfer.</p>



<p>Here is the translation of the foregoing tech speak: “<em>Please follow privacy laws when using A2A and don’t send a lot of sensitive data</em>.”&nbsp;Google’s sum total of privacy protections for the A2A Protocol is basically the Vulcan valediction: “Live Long and Prosper (and, pretty please, don’t violate ‘dem privacy laws).”&nbsp;As noted at length above, there are no mechanisms for transparency or control relative to privacy, and there is no method for making privacy representations, much less for enforcing them.</p>



<p>Any enterprise application and any consumer-facing application that embeds the A2A Protocol would do well to have a comprehensive static and dynamic analysis of the relevant code base to run-to-ground any potential A2A issues, because there are probably going to be a bunch….</p>



<p><strong><u>Agent Card Structure</u></strong></p>



<figure style=" max-width: 100%; height: auto; " class="wp-block-image size-large"><img style=" max-width: 100%; height: auto; " decoding="async" width="656" height="369" src="https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-656x369.png" alt="" class="wp-image-6559" srcset="https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-656x369.png 656w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-320x180.png 320w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-240x135.png 240w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-768x432.png 768w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-40x22.png 40w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-80x45.png 80w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-160x90.png 160w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-550x309.png 550w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-367x206.png 367w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-734x413.png 734w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-275x155.png 275w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-825x464.png 825w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-220x124.png 220w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-440x247.png 440w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-660x371.png 660w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-880x495.png 880w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-184x103.png 184w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-917x516.png 917w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-138x78.png 138w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-413x232.png 413w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-688x387.png 688w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-123x69.png 123w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-110x62.png 110w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-330x186.png 330w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-300x169.png 300w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-600x337.png 600w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-207x116.png 207w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-344x193.png 344w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-55x31.png 55w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-71x40.png 71w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image-96x54.png 96w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/image.png 962w" sizes="(max-width: 656px) 100vw, 656px" /></figure>



<p>About the Author:</p>



<p><a href="steven.roosa@nortonrosefulbright.com">Steven B. Roosa</a>, a partner in the Norton Rose Fulbright&#8217;s New York office, created <a href="https://www.ntanalyzer.com/">NT Analyzer</a>, the firm’s privacy testing tool suite that uses network traffic analysis, and he actively develops and evaluates AI applications for various use-cases.</p>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>New Jersey’s proposed privacy rules include some surprises</title>
		<link>https://www.dataprotectionreport.com/2025/06/new-jerseys-proposed-privacy-rules-include-some-surprises/</link>
		
		<dc:creator><![CDATA[David Kessler (US) and Susan Ross (US)]]></dc:creator>
		<pubDate>Tue, 24 Jun 2025 10:33:31 +0000</pubDate>
				<category><![CDATA[Compliance and risk management]]></category>
		<category><![CDATA[new jersey]]></category>
		<category><![CDATA[Privacy]]></category>
		<category><![CDATA[regulations]]></category>
		<guid isPermaLink="false">https://www.dataprotectionreport.com/?p=6555</guid>

					<description><![CDATA[On June 2, 2025, the New Jersey Attorney General’s Division of Consumer Affairs released proposed rules (57 N.J.R. 1101(a)) pursuant to the New Jersey Data Privacy Act (N.J.S.A. 56:8-166.4 et seq.).&#160; Although the proposed rules have many similarities to California’s current privacy regulations, there are some surprises.&#160; Among other things, New Jersey has proposed a... <a href="https://www.dataprotectionreport.com/2025/06/new-jerseys-proposed-privacy-rules-include-some-surprises/">Continue Reading</a>]]></description>
										<content:encoded><![CDATA[

<p>On June 2, 2025, the New Jersey Attorney General’s Division of Consumer Affairs <a href="https://advance.lexis.com/documentpage/?pdmfid=1000516&amp;crid=1e0cc76e-1865-4aee-909a-28f4f6d52b81&amp;nodeid=AABAABAABAACAADAAB&amp;nodepath=%2fROOT%2fAAB%2fAABAAB%2fAABAABAAB%2fAABAABAABAAC%2fAABAABAABAACAAD%2fAABAABAABAACAADAAB&amp;level=6&amp;haschildren=&amp;populated=false&amp;title=57+N.J.R.+1101(a)&amp;config=025154JABiMmFjYzAxMy1hNjIyLTQ0YTctOTY0NS1iOGNlMTRiYzBkNGQKAFBvZENhdGFsb2flnvGwky16hNN9rcMfcun6&amp;pddocfullpath=%2fshared%2fdocument%2fadministrative-codes%2furn%3acontentItem%3a6FW0-81S3-RVH8-Y16R-00008-00&amp;ecomp=6gf5kkk&amp;prid=deaf4a3a-c72d-458f-8a9e-7637ff41fc10">released</a> proposed rules (57 N.J.R. 1101(a)) pursuant to the New Jersey Data Privacy Act (N.J.S.A. 56:8-166.4 et seq.).&nbsp; Although the proposed rules have many similarities to California’s <a href="https://cppa.ca.gov/regulations/pdf/cppa_regs.pdf">current privacy regulations</a>, there are some surprises.&nbsp; Among other things, New Jersey has proposed a limited definition of the “internal research” exception to the rules that excludes using personal data to train AI:</p>



<p class="is-style-indented">conduct is not &#8220;internal research&#8221; if: (1) the data or resulting research is shared with a third party, unless it is de-identified or shared pursuant to N.J.A.C. 13:45L-1.3(c); or (2) the data or resulting research is used to train artificial intelligence, unless the consumer has affirmatively consented to such use.</p>



<p>(emphasis supplied)&nbsp; The proposed rules may also have been influenced by a recent California Privacy Protection Agency settlement (see our blog post <a href="https://www.dataprotectionreport.com/2025/05/the-california-privacy-protection-agency-may-be-clicking-through-your-website/">here</a>), because the proposed rules would also require that controllers “test their methods for submitting data right requests and obtaining consumer consent to ensure they are functional and do not undermine their consumers&#8217; choices.”&nbsp; In other words, the regulation would mandate that the controller have technology for consumers to submit data rights requests, and must also test that technology to make sure it works.</p>



<p>In addition, similar to other states (such as Texas), the proposed rules require certain disclosures if the consumer’s personal data will be used for “profiling” if the profiles result in “decisions that produce legal or similarly significant effects concerning the consumer.”&nbsp; The rule would define “profiling” as “any form of automated processing performed on personal data to evaluate, analyze, or predict personal aspects related to an identified or identifiable individual&#8217;s economic situation, health, personal preferences, interests, reliability, behavior, location, or movements.”&nbsp; New Jersey would mandate the following disclosures:</p>



<p class="is-style-indented">(1) the categories of personal data that will be processed as part of the profiling; (2) the decisions that are made by using profiling; (3) <strong>whether the system has been evaluated for accuracy, fairness, or bias</strong>; and (4) information about how a consumer may exercise the right to opt out of the processing of personal data for profiling in furtherance of decisions that produce legal or other similarly significant effects.</p>



<p>(emphasis supplied).&nbsp;</p>



<p>New Jersey would also place an affirmative obligation on controllers “to notify consumers of material changes to their privacy notice in a manner by which the controllers regularly interact with consumers.”&nbsp; The controller would be required to obtain the consumer’s affirmative consent prior to processing any personal information under the new revised privacy notice.</p>



<p>The proposed rules would also require controllers to “ensure” that processors make corrections in response to a consumer’s request to correct the personal information.&nbsp; It is unclear whether this obligates controllers to do anything beyond including necessary requirements in processing contracts.</p>



<p>Unlike California’s requirements, if a consumer makes a request to delete personal information, the controller would be required to delete information about that consumer it obtained from a source other than the consumer.</p>



<p>The controller’s privacy policy must specify the purpose for each processing activity.&nbsp; The proposed rule would prohibit controllers “from identifying one broad purpose to justify numerous processing activities, specifying one broad purpose to cover potential future processing activities, or identifying so many purposes for which personal data could be processed, that the purpose or purposes become unclear or uninformative.”&nbsp; The proposed regulation includes some examples of descriptions:&nbsp; &#8220;targeted advertising,&#8221; &#8220;credit profiling,&#8221; or &#8220;AI modeling.&#8221;</p>



<p>New Jersey would also require controllers to take certain steps with respect to data:</p>



<ul class="wp-block-list">
<li>Create, establish, update, and maintain a data inventory documenting the types of data that the controller possesses, where the data is stored, and who has access to the data;</li>



<li>At least once a year, assess whether biometric identifiers, photographs depicting one or more persons, audio or voice recordings containing the voice of one or more persons, or any personal data generated from a photograph or an audio or video recording held by a controller is still necessary for the specific processing purpose or purposes, and document such assessment . . .</li>



<li>To ensure that the personal data is not kept longer than necessary, a controller shall set reasonable, specific time limits for erasure or for conducting a periodic review.</li>
</ul>



<p>On a related retention point, the rules would also place a time limit on consent for some types of consumers:&nbsp; “When a consumer has not interacted with a controller in the prior 24 months, the controller shall refresh consent.”</p>



<p>New Jersey also proposes this transition requirement:</p>



<p class="is-style-indented">If a controller has collected personal data prior to (the effective date of this rulemaking), and the processing purpose changes after (the effective date of this rulemaking), such that the new purpose is neither reasonably necessary to, nor compatible with, the purposes for which such personal data was processed, as disclosed to the consumer, the controller must obtain valid consent before the time the processing purpose changes to continue to process the previously collected personal data.</p>



<p>Similar to other states, New Jersey would require an annual data protection assessment for any use of personal data that presents a “heightened risk or harm to a consumer.”&nbsp; If the data is used for profiling, the controller would be required to update the assessment annually, or more frequently if “existing processing activities are modified in a way that materially changes the level of risk presented” or a new data processing activity.&nbsp; Among the modifications that “may materially change the level of risk of a processing activity may include changes to . . . the algorithm applied.”&nbsp; Given the frequency with which artificial intelligence algorithms may change, this proposal could place a significant burden on controllers.</p>



<p><strong><u>Our Take</u></strong></p>



<p>Although not revolutionary, the proposed rules would create obligations around the management and processing of consumer personal data that will require careful planning before they can be successfully implemented.&nbsp; For example, the requirement that consent be refreshed if the controller has not be in touch with the consumer for 24 months, either requires refreshing consent every 24 months or keeping track of consumer interactions.&nbsp; Likewise, given the prevalence of audio recordings for helplines and the use of video recordings for security purposes, many companies that operate in New Jersey will need to develop yearly plans to evaluate their use and retention&nbsp; In addition, data controllers will need to evaluate (and document) any profiling of consumers for “accuracy, fairness, or bias”.</p>



<p>Perhaps most problematic is the requirement that controllers “create, establish, update, and maintain a data inventory documenting the types of data that the controller possesses, where the data is stored, and who has access to the data, ” as such inventories are notoriously difficult to create and maintain as organizations evolve and their IT systems grow.&nbsp; Depending on how the Division of Consumer Affairs interprets how granular the requirement is, this section could be a significant compliance burden.</p>



<p>It should also be noted that several of the proposed New Jersey privacy rule provisions would affect uses of consumer personal data in artificial intelligence that could have impacts well beyond New Jersey.&nbsp;</p>



<p>For people or entities who want to comment on the Proposed Rules, comments are due by August 2, 2025.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>AI and Job Postings: Navigating Ontario’s Upcoming Requirements</title>
		<link>https://www.dataprotectionreport.com/2025/06/ai-and-job-postings-navigating-ontarios-upcoming-requirements/</link>
		
		<dc:creator><![CDATA[Imran Ahmad (CA), Domenic Presta (CA), Joseph Cohen-Lyons and Humna Shaikh]]></dc:creator>
		<pubDate>Mon, 16 Jun 2025 20:19:13 +0000</pubDate>
				<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Compliance and risk management]]></category>
		<category><![CDATA[Data Protection]]></category>
		<category><![CDATA[Regulatory response]]></category>
		<category><![CDATA[artificial intelligence]]></category>
		<category><![CDATA[data privacy]]></category>
		<category><![CDATA[data protection]]></category>
		<guid isPermaLink="false">https://www.dataprotectionreport.com/?p=6538</guid>

					<description><![CDATA[On March 21, the Ontario’s Bill 149, Working for Workers Four Act, 2024 (“Bill 149”) received Royal Assent.  <a href="https://www.dataprotectionreport.com/2025/06/ai-and-job-postings-navigating-ontarios-upcoming-requirements/">Continue Reading</a>]]></description>
										<content:encoded><![CDATA[<img style=" max-width: 100%; height: auto; " width="1100" height="438" src="https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-1100x438.png" class="attachment-lxb_af_1_of_1 size-lxb_af_1_of_1 wp-post-image" alt="" decoding="async" loading="lazy" srcset="https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-1100x438.png 1100w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-320x127.png 320w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-656x261.png 656w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-240x96.png 240w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-768x306.png 768w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-1536x612.png 1536w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-2048x816.png 2048w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-40x16.png 40w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-80x32.png 80w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-160x64.png 160w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-2200x876.png 2200w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-550x219.png 550w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-367x146.png 367w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-734x292.png 734w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-275x110.png 275w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-825x329.png 825w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-220x88.png 220w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-440x175.png 440w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-660x263.png 660w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-880x351.png 880w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-184x73.png 184w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-917x365.png 917w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-138x55.png 138w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-413x165.png 413w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-688x274.png 688w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-963x384.png 963w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-123x49.png 123w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-110x44.png 110w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-330x131.png 330w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-300x120.png 300w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-600x239.png 600w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-207x82.png 207w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-344x137.png 344w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-55x22.png 55w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-71x28.png 71w, https://www.dataprotectionreport.com/wp-content/uploads/sites/15/2025/06/Print-AdobeStock_788399040-136x54.png 136w" sizes="auto, (max-width: 1100px) 100vw, 1100px" />

<p>On March 21, the Ontario’s Bill 149, <em>Working for Workers Four Act</em>, 2024<a href="#_ftn1" id="_ftnref1">[1]</a> (“<strong>Bill 149</strong>”) received Royal Assent. It introduced significant amendments to the <em>Employment Standards Act</em>, 2000 (“<strong>ESA</strong>”), including a mandate for employers to disclose the use of artificial intelligence (“<strong>AI</strong>”) in publicly advertised job postings. As of January 1, 2026, employers with twenty-five (25) or more employees must include a statement on job postings if AI is used to screen, assess, or select applicants.</p>



<p>As employers prepare for this change, compliance should align hiring practices with broader legal and ethical standards. Transparency and fairness are important considerations associated with the responsible use of AI, and Ontario’s mandatory disclosure requirements take these into account. By being mindful of transparency and fairness when using AI, employers can reduce the risk of running afoul of disclosure requirements while demonstrating a commitment to ethical hiring practices in an increasingly automated landscape.</p>



<p><strong>AI Disclosure Requirements Under Bill 149</strong></p>



<p>Bill 149 amends the ESA by introducing several new requirements, including requirements with respect to AI, which it defines broadly as “<em>a machine-based system that, for explicit or implicit objectives, infers from the input it receives in order to generate outputs such as predictions, content, recommendations or decisions that can influence physical or virtual environment.</em>”<a id="_ftnref2" href="#_ftn2">[2]</a></p>



<p>With respect to AI, Bill 149 provides that employers must disclose if AI is used in assessing or selecting candidates in any publicly posted job.</p>



<p>For a detailed summary of the various changes implemented by Bill 149, see our blog post: <a href="https://www.nortonrosefulbright.com/en/knowledge/publications/68599b7e/ontarios-working-for-workers-four-act-receives-royal-assent">Ontario’s Working for Workers Four Act receives royal assent | Global law firm | Norton Rose Fulbright</a>.</p>



<p><strong>Scope and Exceptions</strong></p>



<p>These rules apply to external job postings accessible to the public. Notably, Bill 149 exempts:</p>



<ul class="wp-block-list">
<li>Internal postings (for current employees only).</li>



<li>General “help wanted” signs.</li>



<li>Recruitment for work performed outside Ontario.</li>
</ul>



<p>Employers with fewer than twenty-five (25) employees are not currently subject to these obligations.<a href="#_ftn3" id="_ftnref3">[3]</a></p>



<p><strong>Legal and Human Rights Considerations</strong></p>



<p>Automated decision-making is increasingly being used by organizations to attract top talent, streamline the hiring process or aid in performance evaluations.<a href="#_ftn4" id="_ftnref4">[4]</a> However, potential impact of algorithmic bias and the need for transparency generally remain important considerations around the fair and responsible use of AI.</p>



<p>The Ontario Human Rights Commission (OHRC) has flagged AI use in employment as a growing risk, citing the potential for indirect discrimination through algorithmic bias.<a href="#_ftn5" id="_ftnref5">[5]</a> For example, AI tools trained on historical hiring data could replicate unfair requirements and biased language in job advertisements. This could unintentionally favor certain demographics and exclude others with certain characteristics, thereby potentially infringing rights under the Ontario <em>Human Rights Code</em> (the “<strong>Code</strong>”).</p>



<p>This means failure to adhere to Bill 149’s AI disclosure requirements may not only trigger ESA enforcement but also human rights complaints under the Code, particularly if AI-driven decisions lead to discriminatory outcomes.</p>



<p><strong>Practical Risk Mitigation Strategies</strong></p>



<p>Although a regulation accompanying Bill 149 was released in late 2024<a href="#_ftn6" id="_ftnref6">[6]</a> and provides a definition of AI, it does not clarify which specific tools, systems, or processes fall within this definition and would therefore trigger the disclosure requirement for publicly advertised job postings.</p>



<p>The current lack of clarity around what constitutes AI use in job postings under Bill 149 may lead to inconsistent reporting (either underreporting or overreporting) which could inadvertently increase the risk of non-compliance. In the absence of further guidance from the Government of Ontario and given the AI disclosure requirements that are expected to take effect next January, employers should use the intervening months to prepare for compliance and consider taking the following steps:</p>



<ul class="wp-block-list">
<li><strong>Conduct Algorithmic Impact Assessments: </strong>Regular audits can identify biased outputs or data gaps in AI tools. Employers should document AI decision logic and test for discriminatory patterns.</li>



<li><strong>Develop Transparent Disclosure Practices: </strong>While Bill 149 does not clarify the exact language to meet the AI disclosure requirements, to err on the side of caution, job postings should clearly state how AI is used in the recruitment and onboarding process, and at what stages.</li>



<li><strong>Integrate Human Oversight: </strong>Employers already using AI should consider a “human-in-the-loop” approach for hiring practices. This helps ensure final hiring decisions are reviewed and contextualized by HR professionals in order to limit overreliance on algorithmic judgment and prevent undesired results (for example, inadvertently excluding certain demographics from the hiring pool due to poor training data relied-upon by AI models). The OHRC also recommends that employers demonstrate a reasonable degree of transparency associated with the AI solutions that are leveraged during the course of the hiring process.</li>



<li><strong>Train HR and Legal Teams: </strong>Staff should be possessed of a clear understanding of how AI is deployed by their organizations, as well as having a firm grasp of what their associated legal disclosure and privacy obligations are. This includes reviewing and auditing existing practices and policies and conducting gap analyses to prepare for new obligations under Bill 149, such as the adoption of an AI governance framework for the responsible use of AI.</li>



<li><strong>Review Vendor Agreements: </strong>If an employer is using third-party AI solutions to assist with hiring, it would be advisable to confirm that such solutions will be compliant with Bill 149, and to the extent there are compliance gaps, remediate same by way of contractual amendments.</li>
</ul>



<p><strong>Key Takeaways</strong></p>



<p>Ontario’s Bill 149 introduces new AI disclosure requirements that reflect the growing role of AI use in hiring practices. As these rules come into effect, businesses should begin assessing how AI is used in their recruitment processes and take steps to align with evolving legal expectations. While the focus is on transparency and fairness, these principles now carry legal weight across employment, human rights, and privacy frameworks. This is particularly important in the context of a gatekeeping function such as the hiring of employees. Employers must remain aware of and make efforts to mitigate against the risks associated with AI systems that may have a material adverse impact on individuals, whether by act or omission. Preparing early can help organizations navigate these changes smoothly and responsibly.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p><a href="#_ftnref1" id="_ftn1">[1]</a> <a href="https://www.ola.org/en/legislative-business/bills/parliament-43/session-1/bill-149">Bill 149, Working for Workers Four Act, 2024</a>.</p>



<p><a href="#_ftnref2" id="_ftn2">[2]</a> <a href="https://www.ontario.ca/laws/regulation/r24476">Section 2(1), Employment Standards Act, Ontario regulation 476/24, 2024</a>.</p>



<p><a href="#_ftnref3" id="_ftn3">[3]</a> <a href="https://www.ontario.ca/laws/regulation/r24476">Section 2(1), Employment Standards Act, Ontario regulation 476/24, 2024</a>.</p>



<p><a href="#_ftnref4" id="_ftn4">[4]</a> <a href="https://www.theglobeandmail.com/business/article-ai-eases-the-burden-of-repetitive-hr-work-but-the-human-touch-is-still/">AI eases the burden of repetitive HR work, but the human touch is still needed, Globe and Mail (2024)</a>.</p>



<p><a href="#_ftnref5" id="_ftn5">[5]</a> <a href="https://www3.ohrc.on.ca/en/news-center/ontario-human-rights-commission-submission-standing-committee-social-policy-regarding">Ontario Human Rights Commission Submission to the Standing Committee on Social Policy Regarding Bill 149, Working for Workers Four Act, 2023</a>.</p>



<p><a href="#_ftnref6" id="_ftn6">[6]</a> <a href="https://www.ontario.ca/laws/regulation/r24476">Section 2(1), Employment Standards Act, Ontario regulation 476/24, 2024</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>FTC’s COPPA Rule changes include AI training consent requirement</title>
		<link>https://www.dataprotectionreport.com/2025/06/ftcs-coppa-rule-changes-include-ai-training-consent-requirement/</link>
		
		<dc:creator><![CDATA[David Kessler (US) and Susan Ross (US)]]></dc:creator>
		<pubDate>Wed, 04 Jun 2025 05:00:00 +0000</pubDate>
				<category><![CDATA[COPPA]]></category>
		<category><![CDATA[AI]]></category>
		<category><![CDATA[artificial intelligence]]></category>
		<category><![CDATA[FTC]]></category>
		<guid isPermaLink="false">https://www.dataprotectionreport.com/?p=6530</guid>

					<description><![CDATA[The Federal Trade Commission has published a Final Rule relating to changes in the Children’s Online Privacy Protection Act (“COPPA”) regulations, which will go into effect on Monday, June 23, 2025. &#160;The final Rule generally provides 365 days from the final Rule&#8217;s publication date (April 22, 2025) to come into full compliance.&#160; The Final Rule... <a href="https://www.dataprotectionreport.com/2025/06/ftcs-coppa-rule-changes-include-ai-training-consent-requirement/">Continue Reading</a>]]></description>
										<content:encoded><![CDATA[

<p>The Federal Trade Commission has published a <a href="https://www.govinfo.gov/content/pkg/FR-2025-04-22/html/2025-05904.htm">Final Rule</a> relating to changes in the Children’s Online Privacy Protection Act (“COPPA”) regulations, which will go into effect on Monday, June 23, 2025. &nbsp;The final Rule generally provides 365 days from the final Rule&#8217;s publication date (April 22, 2025) to come into full compliance.&nbsp; The Final Rule contains some substantive changes (including a prohibition of indefinite retention of a child’s data) but also includes some commentary that may be of interest (including a consent requirement for use in AI training).</p>



<p><span style="text-decoration: underline">Background</span></p>



<p>Congress passed COPPA in 1998, and the Federal Trade Commission promulgated regulations in 1999 relating to children under 13 and the personal information that websites and apps could collect and to require parental consent for that collection and use.&nbsp;&nbsp; The FTC amended the COPPA Rule in 2013.&nbsp; The FTC then proposed amendments in 2019, which were delayed by COVID, but have now been finalized.</p>



<p>COPPA Rule Changes</p>



<p>Unlike the 2013 changes, the latest changes are few in number.&nbsp; For example, the FTC will now permit websites/apps to obtain parental consent via text messages in order to collect and share a child’s personal information, as long as those text messages are coupled with additional steps to provide assurances that the person providing the consent is the parent and not the child. Those additional steps include: “Sending a confirmatory text message to the parent following receipt of consent, or obtaining a postal address or telephone number from the parent and confirming the parent&#8217;s consent by letter or telephone call.”&nbsp; The notice must state that the parent can revoke any consent given in response to the earlier text message.</p>



<p>The FTC also expanded the definition of “personal information” to include a “biometric identifier that can be used for the automated or semi-automated recognition of an individual, such as fingerprints; handprints; retina patterns; iris patterns; genetic data, including a DNA sequence; voiceprints; gait patterns; facial templates; or faceprints.”</p>



<p>Because many websites and apps are not exclusively directed to children under 13, the question of whether COPPA would apply to a particular website or app is a multi-factor analysis.&nbsp; The FTC has now added four more factors to aid in determining whether the site/app is directed to children:&nbsp; “competent and reliable empirical evidence regarding &nbsp;. . . marketing or promotional materials or plans, representations to consumers or to third parties, reviews by users or third parties, and the age of users on similar websites or services.”</p>



<p>The FTC also expanded the information available to parents, as well as parental choices.&nbsp; If the website/app discloses personal information to third parties, the disclosure in the direct notice to the parent must include not only the purpose of the disclosure but also the identity or specific category of the third parties that would receive the data.&nbsp; The FTC made it clear that a parent can decide to consent to the collection but not the sharing, or both.&nbsp;</p>



<p>On a related note, the FTC will require that the website notice must include</p>



<ul class="wp-block-list">
<li>identities and specific categories of any third parties to which the operator discloses personal information;</li>



<li>the purposes for such disclosures;</li>



<li>the operator&#8217;s data retention policy for such data;</li>



<li>if applicable, the specific internal operations for which the operator has collected a persistent identifier for the child; and</li>



<li>where the operator collects audio files containing a child&#8217;s voice and no other personal information, a description of how the operator uses such audio files and that the operator deletes such audio files immediately after responding to the request for which they were collected. </li>
</ul>



<p>The FTC also added several requirements relating to security of the data, including (a) maintaining a written information security program; (b) designating one or more employees to coordinate that program; and (c) conducting an annual assessment of internal and external risks.&nbsp; In addition, before allowing other operators, service providers, or third parties to collect or maintain personal information from children, the operator of the website/app must “determine that such entities are capable of maintaining the confidentiality, security, and integrity of the information and must obtain written assurances that such entities will employ reasonable measures to maintain the confidentiality, security, and integrity of the information.”&nbsp; The FTC stated in the commentary that these requirements are modeled on the FTC’s Safeguards Rule.</p>



<p>With respect to retention, the FTC has amended that section of the rule to read:</p>



<p class="is-style-indented">&nbsp;&nbsp;&nbsp; An operator of a website or online service shall retain personal information collected online from a child for only as long as is reasonably necessary to fulfill the specific purpose(s) for which the information was collected. When such information is no longer reasonably necessary for the purposes for which it was collected, the operator must delete the information using reasonable measures to protect against unauthorized access to, or use of, the information in connection with its deletion. <strong><u>Personal information collected online from a child may not be retained indefinitely</u></strong>. At a minimum, the operator must establish, implement, and maintain a written data retention policy that sets forth the purposes for which children&#8217;s personal information is collected, the business need for retaining such information, and a timeframe for deletion of such information. The operator must provide its written data retention policy addressing personal information collected from children in the notice on the website or online service . . .</p>



<p>(emphasis supplied).&nbsp; More specifically, the FTC requires “a hyperlink to the operator&#8217;s online notice that must describe the business need for retaining children&#8217;s personal information and the timeframe for deleting it.”&nbsp; In the commentary, the FTC pointed out that “if operators are not deleting that information as required, then they will be liable for that failure under the relevant provision of the Rule.”</p>



<p><span style="text-decoration: underline">FTC Commentary</span></p>



<p>In addition to the Rule changes themselves, the FTC’s 130+ pages of commentary in the Federal Register notice also contained some noteworthy insights:</p>



<p>AI Training.&nbsp; “Disclosures of a child&#8217;s personal information to third parties for monetary or other consideration, for advertising purposes, or to train or otherwise develop artificial intelligence technologies, are not integral to the website or online service and would require consent”</p>



<p>Biometrics.&nbsp; “The Commission also expects that biometric identifiers, particularly when combined with increasingly sophisticated methods of consumer profiling, potentially could be used to track and deliver targeted advertisements to specific children online, which would constitute online contact.&#8221;&nbsp; The FTC also found “as some commenters noted, storage of sensitive biometric identifiers for even limited periods of time increases the risk that such data will be compromised in a data security incident.”</p>



<p><span style="text-decoration: underline">Our Take</span></p>



<p>Unsurprisingly, children’s data is the subject of heightened regulatory attention.&nbsp; The focus on data retention is also not new as the FTC, like many other regulators, have realized that <a href="https://www.dataprotectionreport.com/2024/02/two-ftc-complaints-that-over-retention-of-personal-data-violates-section-5/">over retention</a> of personal data exacerbates cyber breaches and data that that no longer exists cannot <a href="https://www.dataprotectionreport.com/2024/02/two-ftc-complaints-that-over-retention-of-personal-data-violates-section-5/"></a>be misused.&nbsp; Although the FTC or other regulator is not likely to audit a company to determine compliance with the personal information disposal requirements, regulators have starting issuing <a href="https://www.dataprotectionreport.com/2024/01/8-million-penalty-to-nydfs-and-another-case-of-over-retention/">fines</a> for over-retention when a company has a security incident.&nbsp; As the FTC makes clear here, violations of the destruction requirements is a violation of the COPPA Rule and, in 2025, would be subject to a fine of $53,088 per violation.</p>



<p>With respect to AI, the FTC did not explain whether it considered the parental consent requirement would also apply to children’s data collected prior to the effective date that is used for AI training, nor did the FTC describe what a company must do if a parent withdraws consent and the data is already in the AI algorithm.&nbsp; The FTC also did not address how the data disposal requirements would apply to that training data.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Generative AI: Updated global guide to key IP considerations</title>
		<link>https://www.dataprotectionreport.com/2025/06/generative-ai-updated-global-guide-to-key-ip-considerations/</link>
		
		<dc:creator><![CDATA[Jonathan Ball (UK), Felicia Boyd (US), Justin Davidson (HK), Georgina Hey (AU), Jurriaan Jansen, Mike Knapper (UK), Frank Liu, Maya Medeiros, Clemens Rübel, Allison Williams, Jeremiah Chew, Jasper Geerdes, Ronak kalhor-witzel, Amy King and Clément Monnet]]></dc:creator>
		<pubDate>Tue, 03 Jun 2025 08:49:08 +0000</pubDate>
				<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Intellectual Property]]></category>
		<guid isPermaLink="false">https://www.dataprotectionreport.com/?p=6527</guid>

					<description><![CDATA[Generative AI systems are trained using vast amounts of data, often taken from sources in the public domain that may be protected by copyright or other intellectual property rights. So could training a generative AI system using publicly accessible copyright work constitute an infringement? And could the output infringe? Our global guide has just been... <a href="https://www.dataprotectionreport.com/2025/06/generative-ai-updated-global-guide-to-key-ip-considerations/">Continue Reading</a>]]></description>
										<content:encoded><![CDATA[

<p>Generative AI systems are trained using vast amounts of data, often taken from sources in the public domain that may be protected by copyright or other intellectual property rights. So could training a generative AI system using publicly accessible copyright work constitute an infringement? And could the output infringe?</p>



<p>Our global guide has just been updated and republished and looks at the current legal situation across a wide range of different jurisdictions – including Australia, Canada, China, EU, France, Germany, Hong Kong, The Netherlands, Singapore, South Africa, UK and US. It covers issues such as:</p>



<ul class="wp-block-list">
<li>Infringement risk relating to training a generative AI system.</li>



<li>Loss of confidentiality in the information used as the &#8216;prompt’ for a generative AI system.</li>



<li>Is the output of the generative AI system protected by intellectual property rights?</li>



<li>Infringement risk relating to creation and use of the output of a generative AI system.</li>



<li>Which ‘actors’ have potential liability for infringement?</li>
</ul>



<p>Access the full guide <a href="https://www.nortonrosefulbright.com/en/knowledge/publications/acf01adf/generative-ai">here</a>.</p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>AI literacy – the Commission’s pointers on building your programme</title>
		<link>https://www.dataprotectionreport.com/2025/05/ai-literacy-the-commissions-pointers-on-building-your-programme/</link>
		
		<dc:creator><![CDATA[Marcus Evans (UK) and Rosie Nance]]></dc:creator>
		<pubDate>Wed, 28 May 2025 09:43:18 +0000</pubDate>
				<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Compliance and risk management]]></category>
		<guid isPermaLink="false">https://www.dataprotectionreport.com/?p=6523</guid>

					<description><![CDATA[The EU AI Act’s AI literacy obligation applied from 2 February 2025.&#160; This applies to anyone doing anything with AI where there is some connection to the EU – to providers and deployers of any AI systems. The AI Act gives little away on what compliance would look like though. Fortunately, the Commission’s AI Office... <a href="https://www.dataprotectionreport.com/2025/05/ai-literacy-the-commissions-pointers-on-building-your-programme/">Continue Reading</a>]]></description>
										<content:encoded><![CDATA[

<p>The EU AI Act’s AI literacy obligation applied from 2 February 2025.&nbsp; This applies to anyone doing anything with AI where there is some connection to the EU – to providers and deployers of any AI systems.</p>



<p>The AI Act gives little away on what compliance would look like though. Fortunately, the Commission’s AI Office recently provided <a href="https://digital-strategy.ec.europa.eu/en/faqs/ai-literacy-questions-answers" target="_blank" rel="noreferrer noopener">guidance in the form of Questions &amp; Answers</a>, setting out its expectations on AI literacy.</p>



<p><strong>The obligation</strong></p>



<p>Providers and deployers of AI systems must “<em>take measures to ensure, to their best extent, a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems on their behalf</em>” (Article 4).</p>



<p>Recital 20 sums up the requirement as equipping the relevant people with “<em>the necessary notions</em>” to make informed decisions about AI systems.</p>



<p>The definition also refers to making an informed deployment, as well as gaining awareness about the opportunities and risks of AI and possible harm it can cause.</p>



<p><strong>Who needs to be AI literate?</strong></p>



<p>Providers, deployers, and affected persons, as well as staff and other persons dealing with the operation and use of AI systems.</p>



<p>The Commission confirms that it is anyone under the provider’s / deployer’s operational remit, so could be contractors, service providers, or clients.</p>



<p><strong>What is a “sufficient” level of AI literacy?</strong></p>



<p>The Commission will not be imposing strict (or specific) requirements, as this is context-specific.</p>



<p>Organisations need to tailor their approach – for example, organisations using high-risk AI systems might need “<em>additional measures</em>” to ensure that employees understand those risks (and in any event, will need to comply with their Article 26 obligation to ensure staff dealing with AI systems are sufficiently trained to handle the AI system and ensure human oversight).</p>



<p>Where employees only use generative AI, AI literacy training is still needed on relevant risks such as hallucination.</p>



<p>The Commission does not plan to provide sector-specific guidance, although the context in which the AI system is provided or deployed is relevant.</p>



<p>For those who already have a deep technical knowledge, AI literacy training may still be relevant – the organisation should consider whether they understand the risks and how to avoid or mitigate them, and other relevant knowledge such as the legal and ethical aspects of AI.</p>



<p>The Commission points to its <a href="https://digital-strategy.ec.europa.eu/en/library/living-repository-foster-learning-and-exchange-ai-literacy" target="_blank" rel="noreferrer noopener">living repository on AI literacy</a> as a potential source of inspiration.</p>



<p><strong>Is there a “human-in-the-loop” exemption?</strong></p>



<p>No, in fact AI literacy is <em>more</em> important for humans in the loop.&nbsp; To provide genuine oversight, they need to understand the AI systems they are overseeing.</p>



<p><strong>What are the consequences of not doing it?</strong></p>



<p>Enforcement will be by market surveillance authorities and can begin from 2 August 2026 (when the provisions on their enforcement powers come into force).</p>



<p>The Commission includes a question on whether penalties could be imposed for non-compliance from 2 February 2025 when enforcement begins, but does not provide an answer, simply stating that there will be cooperation with the AI Board and all relevant authorities to ensure coherent application of the rules.</p>



<p>The detail on what enforcement will look like is also yet to come.&nbsp; The AI Act does not provide for any specific fines for non-compliance with the AI literacy obligation. In its <a href="https://digital-strategy.ec.europa.eu/en/events/third-ai-pact-webinar-ai-literacy" target="_blank" rel="noreferrer noopener">AI Pact webinar on 20 February 2025</a>, the Commission flagged that although Article 99 AI Act sets maximum penalties in other areas, it does not prevent member states from including specific penalties for non-compliance with the AI literacy obligation in their national laws.&nbsp; The Commission also flagged that AI literacy would be likely to be taken into account following breach of another obligation under the AI Act.</p>



<p>The Commission also mentions the possibility of private enforcement, and individuals suing for damages – but also acknowledges that the AI Act does not create a right to compensation.</p>



<p><strong>Our take</strong></p>



<p>The Commission does not give much away on what AI literacy programmes should look like – but, ultimately, as it highlights, what is “sufficient” will be personal to each organisation.</p>



<p>To shape an AI literacy programme, it will first be necessary to work through:</p>



<ul class="wp-block-list">
<li><strong>Who are the different stakeholders involved in using AI?</strong> This needs to cover everyone – those involved in AI governance, developers, anyone involved in using AI, service providers, clients, and affected persons.</li>



<li><strong>What does each group already know and what does each group need to know?</strong>&nbsp; For example, AI governance committee members may need a deeper understanding of how AI works.&nbsp; Data scientists may need to focus on legal and ethical issues.&nbsp; For employees making occasional use of generative AI, a shorter session on the risks and how the organisation manages them could be appropriate.</li>



<li><strong>What medium would be most appropriate?</strong>&nbsp; E.g. a workshop format might work well for AI governance committee members or data scientists, while an e-learning could be sufficient for employees making occasional use of generative AI.</li>



<li><strong>When will the training be delivered?</strong>&nbsp; As mentioned above, the obligation already applies.</li>



<li><strong>How will we track attendance and ensure that completion is sufficiently high?</strong></li>
</ul>



<p>The Commission’s guidance deals with the specific AI literacy obligation under the AI Act.&nbsp; But really, AI literacy is crucial for all organisations using AI, regardless of whether the AI Act applies. AI literacy is essential for building a strong AI governance programme equipped to manage the range of legal and organisational risks that come with AI use.&nbsp;</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>The California Privacy Protection Agency may be clicking through your website</title>
		<link>https://www.dataprotectionreport.com/2025/05/the-california-privacy-protection-agency-may-be-clicking-through-your-website/</link>
		
		<dc:creator><![CDATA[Steve Roosa (US), Philip Hodgkins (US) and Wenda Tang (US)]]></dc:creator>
		<pubDate>Wed, 21 May 2025 16:57:50 +0000</pubDate>
				<category><![CDATA[NT Analyzer Blog Series]]></category>
		<category><![CDATA[California Privacy Protection Agency]]></category>
		<category><![CDATA[CPPA]]></category>
		<category><![CDATA[personal data]]></category>
		<category><![CDATA[Privacy]]></category>
		<category><![CDATA[Todd Snyder]]></category>
		<guid isPermaLink="false">https://www.dataprotectionreport.com/?p=6519</guid>

					<description><![CDATA[The California Privacy Protection Agency (CPPA) just issued its second enforcement action under the CCPA and the message is clear: the CPPA is looking at your digital properties and tallying up the violations. Your website is more than a marketing tool; it is a compliance obligation. Todd Snyder, a fashion brand under American Eagle, got hit... <a href="https://www.dataprotectionreport.com/2025/05/the-california-privacy-protection-agency-may-be-clicking-through-your-website/">Continue Reading</a>]]></description>
										<content:encoded><![CDATA[

<p>The California Privacy Protection Agency (CPPA) just issued its second enforcement action under the CCPA and the message is clear: the CPPA is looking at your digital properties and tallying up the violations. Your website is more than a marketing tool; it is a compliance obligation.</p>



<p>Todd Snyder, a fashion brand under American Eagle, got hit with a <a href="https://cppa.ca.gov/announcements/2025/20250506.html">$345,000</a> fine for a variety of alleged failures:</p>



<p>• Overcollection of personal data during consumer request processes</p>



<p>• Use of third-party privacy tools that weren’t adequately vetted or monitored</p>



<p>• Malfunctioning opt-out mechanisms</p>



<p>Just a few weeks earlier, one of the world’s major car manufacturers settled for <a href="https://cppa.ca.gov/announcements/2025/20250312.html">$632,500</a> over similar issues. Neither company fits the mold of organizations most typically targeted by enforcement agencies; yet both are now case studies for having your privacy house in order, regardless of who you are.</p>



<p>The CPPA has moved past superficial privacy policy reviews. It is conducting functional assessments of websites and mobile apps. That means the CCPA is:</p>



<ul class="wp-block-list">
<li>Submitting data subject requests as live users</li>



<li>Evaluating your timeline and response protocols</li>



<li>Testing how your tools actually behave</li>



<li>Evaluating your use of third party vendors</li>
</ul>



<p>If your opt-out button doesn’t work, your consent banner vanishes, or your vendor’s tool misfires — you are potentially exposed.</p>



<p>To further highlight this exposure, a recent study conducted by <strong><u>Consumer Reports</u></strong> and Wesleyan University found that <a href="https://innovation.consumerreports.org/Mixed-Signals-Many-Companies-May-Be-Ignoring-Opt-Out-Requests-Under-State-Privacy-Laws.pdf">30% of tested retailers ignored Global Privacy Control opt-out signals,</a> suggesting that many companies are not complying with core consumer protection components of many state and federal regulations despite their privacy policies saying otherwise.</p>



<p>And these shortcomings can be identified by the regulators because they maintain their own, in-house engineering staff and by media outlets like Consumer Reports because they collaborate with research universities. And that is not to mention the class action plaintiff’s attorneys that are conducting their own digital explorations.</p>



<p>That’s where Norton Rose Fulbright’s<a href="https://www.ntanalyzer.com/"> NT Analyzer</a> comes in.</p>



<p>It’s a diagnostic engine for your data collection footprint and opt-out posture. NT Analyzer audits your site, detecting regulatory violations the way engineering professionals at enforcement agencies do — by interacting with your site’s actual data flows and configurations.</p>



<p>With NT Analyzer, Norton Rose Fulbright can help you:</p>



<p>• Identify overcollection of personal information</p>



<p>• Validate the functionality and fairness of consent mechanisms</p>



<p>• Detect data leakage and sensitive data transmissions</p>



<p>The CPPA is no longer focused just on intent; it is measuring execution. And, if your execution is flawed, enforcement is becoming a greater possibility.</p>



<p>On top of these enforcement actions there’s the news that the <a href="https://cppa.ca.gov/announcements/2025/20250416.html">CPPA is teaming up with seven other state regulators</a> to coalesce around consumer protection across jurisdictions, as well as to coordinate investigations into potential violations of laws. It seems certain that digital properties across the board will be facing more scrutiny in the near future.</p>



<p>For more details on how Norton Rose Fulbright can help, please contact the team:</p>



<p>Steve Roosa – <a href="mailto:steven.roosa@nortonrosefulbright.com">steven.roosa@nortonrosefulbright.com</a></p>



<p>Phil Hodgkins – <a href="mailto:phil.hodgkins@nortonrosefulbright.com">phil.hodgkins@nortonrosefulbright.com</a></p>



<p>Wenda Tang – <a href="mailto:wenda.tang@nortonrosefulbright.com">wenda.tang@nortonrosefulbright.com</a></p>



<p></p>



<p></p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
